# Projet : Architecture R√©seau Distribu√©e et S√©curis√©e avec Sauvegarde D√©port√©e

## üåê 1. Architecture et Topologie

Ce projet d√©ploie une infrastructure r√©seau cloisonn√©e simulant un environnement d'entreprise robuste. L'architecture respecte les bonnes pratiques de s√©curit√© : s√©paration des r√¥les (DNS vs Apps), isolation des sauvegardes, et chiffrement des flux.

### 1.1. Inventaire des Machines (VMs)

Le r√©seau Host-Only est : **192.168.142.0/24**.

| Machine | Nom d'H√¥te | R√¥le Principal | IP Statique |
| :--- | :--- | :--- | :--- |
| **VM 1** | **`SRV-DNS`** | Serveur de Noms & Passerelle DNS | **192.168.142.10** |
| **VM 2** | **`SRV-APPS`** | Web (HTTPS) & Fichiers (Samba) | **192.168.142.11** |
| **VM 3** | **`SRV-BACKUP`** | Stockage de Sauvegarde Isol√© | **192.168.142.12** |
| **VM 4** | **`CLI-TEST`** | Client Utilisateur | **192.168.142.20** |

### 1.2. Pr√©requis VirtualBox

  * **Adaptateur 1 :** NAT (Acc√®s Internet).
  * **Adaptateur 2 :** R√©seau H√¥te-Seul (Host-Only Adapter).

-----

## üõ†Ô∏è 2. Configuration IP Initiale (Sur toutes les VMs)

Pour √©viter les conflits d'IP et l'√©crasement par le DHCP, nous for√ßons la configuration statique via `nmcli`.

**Sur VM 1 (SRV-DNS) :**

```bash
sudo hostnamectl set-hostname SRV-DNS
sudo nmcli con add type ethernet ifname enp0s8 con-name static-hostonly ip4 192.168.142.10/24 autoconnect yes
sudo nmcli con up static-hostonly
```

**Sur VM 2 (SRV-APPS) :**

```bash
sudo hostnamectl set-hostname SRV-APPS
sudo nmcli con add type ethernet ifname enp0s8 con-name static-hostonly ip4 192.168.142.11/24 autoconnect yes
sudo nmcli con up static-hostonly
```

**Sur VM 3 (SRV-BACKUP) :**

```bash
sudo hostnamectl set-hostname SRV-BACKUP
sudo nmcli con add type ethernet ifname enp0s8 con-name static-hostonly ip4 192.168.142.12/24 autoconnect yes
sudo nmcli con up static-hostonly
```

**Sur VM 4 (CLI-TEST) :**

```bash
sudo hostnamectl set-hostname CLI-TEST
sudo nmcli con add type ethernet ifname enp0s8 con-name static-hostonly ip4 192.168.142.20/24 autoconnect yes
sudo nmcli con up static-hostonly
```

-----

## üß† 3. VM 1 : SRV-DNS (Le Cerveau du R√©seau)

Il r√©sout les noms locaux (`.lan`) et transmet les requ√™tes inconnues vers Internet (*Forwarding*), permettant aux autres serveurs (comme Backup) de faire leurs mises √† jour.

### 3.1. Installation et Pare-feu

```bash
sudo dnf install bind bind-utils -y
sudo systemctl enable --now firewalld
sudo firewall-cmd --permanent --add-port={53/tcp,53/udp}
sudo firewall-cmd --reload
```

### 3.2. Configuration BIND (`/etc/named.conf`)

Modifiez les options pour √©couter sur le r√©seau et activer le forwarding.

```bash
sudo vi /etc/named.conf
```

**Modifications cl√©s :**

```conf
options {
    listen-on port 53 { 127.0.0.1; 192.168.142.10; }; # Ajouter l'IP locale
    allow-query     { localhost; 192.168.142.0/24; }; # Autoriser le r√©seau
    
    # Activer le forwarding vers Google (pour que les VMs aient internet via le DNS)
    forward only;
    forwarders { 8.8.8.8; };
};

# Ajouter la zone √† la fin :
zone "monlabo.lan" IN {
    type master;
    file "db.monlabo.lan";
    allow-update { none; };
};
```

### 3.3. Fichier de Zone (`/var/named/db.monlabo.lan`)

```bash
sudo cp /var/named/named.localhost /var/named/db.monlabo.lan
sudo vi /var/named/db.monlabo.lan
```

**Contenu :**

```text
$TTL 86400
@   IN  SOA     srv-dns.monlabo.lan. root.monlabo.lan. (
    2025121102  ; Serial
    3600        ; Refresh
    1800        ; Retry
    604800      ; Expire
    86400 )     ; Minimum TTL

    IN  NS      srv-dns.monlabo.lan.
    IN  A       192.168.142.10

srv-dns     IN  A   192.168.142.10
srv-apps    IN  A   192.168.142.11
srv-backup  IN  A   192.168.142.12

; Services
web         IN  A   192.168.142.11
fichiers    IN  A   192.168.142.11
```

**Finalisation :**

```bash
sudo systemctl enable --now named
sudo systemctl restart named
```

-----

## ‚öôÔ∏è 4. VM 2 : SRV-APPS (Services Web & Fichiers)

### 4.1. Installation et S√©curit√©

```bash
sudo dnf install nginx samba samba-client rsync cronie -y
sudo systemctl enable --now firewalld
sudo firewall-cmd --permanent --add-service={http,https}
sudo firewall-cmd --permanent --add-port=445/tcp
sudo firewall-cmd --reload
```

### 4.2. HTTPS (Nginx)

```bash
# Certificat
sudo mkdir -p /etc/nginx/ssl
sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/labo.key -out /etc/nginx/ssl/labo.crt
# (Configurer nginx.conf pour utiliser ces fichiers sur le port 443)

# SELinux Web
sudo restorecon -Rv /etc/nginx/
sudo systemctl enable --now nginx
```

### 4.3. Partage Samba

```bash
# Utilisateur
sudo useradd [VOTRE-UTILISATEUR]
sudo smbpasswd -a [VOTRE-UTILISATEUR]

# Dossier
sudo mkdir -p /srv/samba/projet
sudo chown [VOTRE-UTILISATEUR]:[VOTRE-UTILISATEUR] /srv/samba/projet
sudo chmod 770 /srv/samba/projet

# Config (/etc/samba/smb.conf)
# Ajouter :
# [ProjetSecret]
#    path = /srv/samba/projet
#    valid users = [VOTRE-UTILISATEUR]
#    read only = no

# SELinux Samba (Critique)
sudo chcon -R -t samba_share_t /srv/samba/projet
sudo setsebool -P samba_enable_home_dirs on
sudo setsebool -P samba_export_all_rw on
sudo systemctl enable --now smb nmb
```

-----

## üíæ 5. VM 3 : SRV-BACKUP (Le Coffre-fort)

Ce serveur est isol√©. Il n'ouvre que le port SSH.

### 5.1. Installation

*Note : Si le DNS n'est pas encore pr√™t, utiliser `sudo nmcli con mod enp0s3 ipv4.dns 8.8.8.8` temporairement pour l'install.*

```bash
sudo dnf install rsync openssh-server -y
sudo systemctl enable --now sshd
```

### 5.2. S√©curit√© Maximale (Pare-feu)

On ferme tout, sauf SSH.

```bash
sudo firewall-cmd --permanent --remove-service={dhcpv6-client,cockpit,http,https}
sudo firewall-cmd --permanent --add-service=ssh
sudo firewall-cmd --reload
```

### 5.3. Dossier de R√©ception

```bash
sudo mkdir -p /mnt/sauvegardes/srv-apps
# Cr√©er l'utilisateur [VOTRE-UTILISATEUR] s'il n'existe pas
sudo useradd [VOTRE-UTILISATEUR]
sudo chown -R [VOTRE-UTILISATEUR]:[VOTRE-UTILISATEUR] /mnt/sauvegardes
sudo chmod -R 770 /mnt/sauvegardes
```

-----

## üîÑ 6. Automatisation de la Sauvegarde (Rsync over SSH)

La sauvegarde est initi√©e par **SRV-APPS** vers **SRV-BACKUP**.

### 6.1. √âchange de Cl√©s SSH (Sur SRV-APPS)

Nous configurons l'utilisateur `root` pour qu'il puisse se connecter sans mot de passe.

```bash
# Sur SRV-APPS
sudo -i  # Passer en root
ssh-keygen -t rsa # (Entr√©e, Entr√©e, Entr√©e)
ssh-copy-id [VOTRE-UTILISATEUR]@192.168.142.12
exit # Quitter root
```

### 6.2. T√¢che Cron (Sur SRV-APPS)

```bash
sudo crontab -e
```

Ajouter la ligne (tous les jours √† 03h00) :

```cron
0 3 * * * /usr/bin/rsync -az --delete /srv/samba/projet/ [VOTRE-UTILISATEUR]@192.168.142.12:/mnt/sauvegardes/srv-apps/ > /dev/null 2>&1
```

-----

## üß™ 7. VM 4 : CLI-TEST (Configuration Client & Tests)

### 7.1. Forcer le DNS Local (NetworkManager)

Il faut emp√™cher l'interface NAT d'√©craser le DNS.

```bash
# 1. Identifier la connexion NAT (ex: enp0s3)
sudo nmcli con mod "enp0s3" ipv4.dns-method "none"
sudo nmcli con mod "enp0s3" ipv4.ignore-auto-dns yes

# 2. Configurer le Host-Only pour utiliser SRV-DNS
sudo nmcli con mod static-hostonly ipv4.dns "192.168.142.10"
sudo nmcli con mod static-hostonly ipv4.dns-search "monlabo.lan"

# 3. Appliquer
sudo nmcli con up "enp0s3"
sudo nmcli con up static-hostonly
```

---

## üì¶ 8. Modernisation : Conteneurisation et Orchestration (Sur SRV-APPS)

Cette section documente le passage d'une infrastructure traditionnelle vers une architecture bas√©e sur les conteneurs. Toutes les manipulations sont effectu√©es sur **SRV-APPS (192.168.142.11)**.

### 8.1. Installation des moteurs de conteneurs

```bash
# Podman & Compose (Alternatif √† Docker)
sudo dnf install podman podman-compose -y

# LXC (Conteneurs Syst√®mes)
sudo dnf install lxc lxc-templates lxc-extra -y

# containerd (Runtime de bas niveau - via d√©p√¥t Docker-CE)
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install containerd.io -y
sudo systemctl enable --now containerd

```

### 8.2. Orchestration Multi-Services (Podman Compose)

D√©ploiement d'une stack Web compl√®te (WordPress + Base de donn√©es) avec persistance des donn√©es.

**Fichier `docker-compose.yml` :**

```yaml
services:
  db:
    image: mariadb:10.6
    volumes:
      - db_data:/var/lib/mysql
    environment:
      MYSQL_DATABASE: wordpress
      MYSQL_USER: user
      MYSQL_PASSWORD: password
      MYSQL_ROOT_PASSWORD: root_password

  wordpress:
    image: wordpress:latest
    ports:
      - "8082:80"
    volumes:
      - wp_data:/var/www/html
    environment:
      WORDPRESS_DB_HOST: db
      WORDPRESS_DB_USER: user
      WORDPRESS_DB_PASSWORD: password
      WORDPRESS_DB_NAME: wordpress
    depends_on:
      - db

volumes:
  db_data:
  wp_data:

```

**Commandes de gestion :**

```bash
podman-compose up -d    # Lancer la stack
podman-compose down     # Arr√™ter (donn√©es conserv√©es dans les volumes)

```

### 8.3. S√©curit√© Rootless et Int√©gration Systemd

Podman permet de g√©rer des **Pods** (groupes de conteneurs partageant le r√©seau `localhost`) sans privil√®ges root, int√©gr√©s directement comme services utilisateur.

```bash
# 1. Cr√©ation d'un Pod
podman pod create --name mon-pod -p 8083:80

# 2. G√©n√©ration des fichiers systemd (Mode Utilisateur)
mkdir -p ~/.config/systemd/user/
podman generate systemd --name mon-pod --files --new
mv *.service ~/.config/systemd/user/

# 3. Activation du service au boot (sans sudo)
systemctl --user daemon-reload
systemctl --user enable --now pod-mon-pod.service

```

### 8.4. Conteneurs Syst√®mes (LXC)

Contrairement aux conteneurs applicatifs, LXC simule un OS complet. Sur Rocky Linux, une configuration r√©seau sp√©cifique est requise.

**Configuration du Pont R√©seau (`lxcbr0`) :**

```bash
sudo nmcli con add type bridge ifname lxcbr0 con-name lxcbr0
sudo nmcli con mod lxcbr0 ipv4.addresses 10.0.3.1/24 ipv4.method manual
sudo nmcli con up lxcbr0

```

**Cycle de vie du conteneur :**

```bash
# Cr√©ation d'un OS Ubuntu 22.04
sudo lxc-create -t download -n ubuntu-test -- --dist ubuntu --release jammy --arch amd64
# Gestion
sudo lxc-start -n ubuntu-test
sudo lxc-attach -n ubuntu-test  # Acc√®s direct au shell systemd

```

### 8.5. Analyse des Runtimes (Bas niveau)

Utilisation de `containerd` via l'outil `ctr` pour manipuler les couches (layers) sans passer par l'abstraction Podman/Docker.

```bash
# T√©l√©chargement manuel
sudo ctr images pull docker.io/library/redis:alpine
# Ex√©cution d'une Task (Processus isol√©)
sudo ctr run -d docker.io/library/redis:alpine mon-redis
sudo ctr tasks ls

```

---

### üß™ 9. Tableau de Validation Final (Architecture Hybride)

| Service | Technologie | Acc√®s / Commande | R√©sultat Attendu |
| --- | --- | --- | --- |
| **Web Wordpress** | Podman | `http://srv-apps:8082` | Page d'installation WP |
| **Pod S√©curis√©** | Podman Rootless | `systemctl --user status` | Service actif (non-root) |
| **OS Ubuntu** | LXC | `lxc-info -n ubuntu-test` | √âtat: **RUNNING** |
| **Runtime** | containerd | `sudo ctr images ls` | Images isol√©es de Podman |

---

## üìä 10. Supervision : Surveillance et Observabilit√©

Cette section documente la mise en place d'une stack de monitoring compl√®te pour surveiller la sant√© des VMs du r√©seau `monlabo.lan`.

### 10.1. Outils de Diagnostic Manuel (Sur toutes les VMs)

Avant l'automatisation, des outils natifs permettent un diagnostic rapide des ressources.

```bash
# Installation des outils de base
sudo dnf install epel-release -y
sudo dnf install htop nethogs -y

# Commandes de v√©rification rapide
htop                # CPU, RAM et Processus (visuel)
df -h               # Occupation des disques
free -h             # M√©moire disponible (colonne 'available')
ss -tuln            # √âtat des ports r√©seaux

```

### 10.2. Centralisation des M√©triques (Stack Prometheus & Grafana)

D√©ploiement d'une stack de supervision sur **SRV-APPS (192.168.142.11)** via Podman-Compose, avec persistance des donn√©es pour Grafana.

**Fichier `~/monitoring/docker-compose.yml` :**

```yaml
services:
  prometheus:
    image: docker.io/prom/prometheus
    container_name: monitoring_prometheus_1
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:Z
      - ./alert.rules.yml:/etc/prometheus/alert.rules.yml:Z
    ports:
      - "9090:9090"

  grafana:
    image: docker.io/grafana/grafana
    container_name: monitoring_grafana_1
    volumes:
      - grafana_data:/var/lib/grafana:Z # Persistance des dashboards
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123

  node-exporter:
    image: quay.io/prometheus/node-exporter
    container_name: monitoring_node-exporter_1
    volumes:
      - /:/host:ro,rslave
    command:
      - '--path.rootfs=/host'
    ports:
      - "9100:9100"

volumes:
  grafana_data: # Volume persistant pour Grafana

```

### 10.3. Configuration de la Collecte (`prometheus.yml`)

Prometheus est configur√© pour collecter les donn√©es de **SRV-APPS** (via le r√©seau interne Podman) et de **SRV-DNS** (via le r√©seau Host-Only).

```yaml
global:
  scrape_interval: 15s

rule_files:
  - "alert.rules.yml" # Chargement des r√®gles d'alerting

scrape_configs:
  - job_name: 'nodes'
    static_configs:
      - targets: 
          - 'node-exporter:9100'        # SRV-APPS
          - '192.168.142.10:9100'      # SRV-DNS

```

![alt text](image-1.png)

### 10.4. Installation de la Sonde sur SRV-DNS (Mode Service)

Sur **SRV-DNS (192.168.142.10)**, l'exportateur est install√© comme un service Systemd pour plus de l√©g√®ret√©.

```bash
# Installation du binaire
sudo mv node_exporter /usr/local/bin/
sudo chmod +x /usr/local/bin/node_exporter
sudo chcon -t bin_t /usr/local/bin/node_exporter # Contexte SELinux

# Cr√©ation du service (/etc/systemd/system/node_exporter.service)
# [Unit] Description=Node Exporter ...
# [Service] ExecStart=/usr/local/bin/node_exporter ...

sudo systemctl enable --now node_exporter
sudo firewall-cmd --permanent --add-port=9100/tcp && sudo firewall-cmd --reload

```

![alt text](image.png)

---

## üö® 11. Alerting Automatique

Mise en place de notifications automatiques bas√©es sur des seuils critiques d√©finis dans `alert.rules.yml`.

### 11.1. D√©finition des R√®gles (`alert.rules.yml`)

```yaml
groups:
  - name: infrastructure_alerts
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 2m
        labels:
          severity: warning

      - alert: DiskAlmostFull
        expr: (1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 95
        for: 1m
        labels:
          severity: critical

```

### 11.2. Simulation et Validation des Alertes

Les alertes ont √©t√© test√©es manuellement pour v√©rifier le passage de l'√©tat **PENDING** √† **FIRING**.

* **Test CPU** : `stress-ng --cpu 2 --timeout 150s` (D√©clenche `HighCPUUsage`).

![alt text](image-3.png)

* **Test Disque** : `sudo fallocate -l 5G /tmp/disk_test` (D√©clenche `DiskAlmostFull`).

![alt text](image-2.png)


---

## üìú 12. Observabilit√© Avanc√©e : Centralisation des Logs avec Loki

Cette section marque le passage d'un monitoring passif (m√©triques) √† une **observabilit√© active**. En int√©grant **Grafana Loki**, l'infrastructure analyse d√©sormais le contexte textuel (logs) des √©v√©nements syst√®me en corr√©lation avec les m√©triques Prometheus.

### Avant de commencer ...

#### Modification du fichier `docker-compose.yml`

```yaml
services:
  prometheus:
    image: docker.io/prom/prometheus
    container_name: monitoring_prometheus_1
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:Z
      - ./alert.rules.yml:/etc/prometheus/alert.rules.yml:Z
    ports:
      - "9090:9090"

  loki:
    image: docker.io/grafana/loki:latest
    container_name: monitoring_loki_1
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml

  grafana:
    image: docker.io/grafana/grafana
    container_name: monitoring_grafana_1
    volumes:
      - grafana_data:/var/lib/grafana:Z
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123

  promtail:
    image: docker.io/grafana/promtail:latest
    container_name: monitoring_promtail_1
    volumes:
      - /var/log:/var/log:ro  # Montage des logs h√¥tes en lecture seule
      - ./promtail-config.yml:/etc/promtail/config.yml:Z
    command: -config.file=/etc/promtail/config.yml

  node-exporter:
    image: quay.io/prometheus/node-exporter
    container_name: monitoring_node-exporter_1
    volumes:
      - /:/host:ro,rslave
    command:
      - '--path.rootfs=/host'
    ports:
      - "9100:9100"

volumes:
  grafana_data:

```

---

### 12.1. Configuration de l'Agent Promtail

Promtail est l'agent responsable de "l'aspiration" des logs sur la machine h√¥te (**SRV-APPS**) pour les envoyer vers le serveur Loki.

**Cr√©ation du fichier `~/monitoring/promtail-config.yml` :**

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
- job_name: system
  static_configs:
  - targets:
      - localhost
    labels:
      job: varlogs
      host: srv-apps
      __path__: /var/log/*log  # Collecte tous les fichiers .log standards

```

### 12.2. Gestion des Permissions (D√©fi SELinux & Rootless)

L'utilisation de **Podman en mode Rootless** impose une contrainte de s√©curit√© : le conteneur n'a pas les privil√®ges root n√©cessaires pour lire les fichiers syst√®mes prot√©g√©s (comme `/var/log/messages` ou `/var/log/nginx/error.log`) ni pour modifier leurs √©tiquettes SELinux.

**Solution appliqu√©e pour la d√©monstration :**
Passage temporaire de SELinux en mode permissif afin d'autoriser la lecture des logs sans bloquer le processus.

```bash
# V√©rification des erreurs (Si permission denied)
podman logs monitoring_promtail_1

# Autorisation temporaire
sudo setenforce 0

```

### 12.3. Analyse via LogQL (Grafana Explore)

L'interrogation des logs se fait via le langage **LogQL** dans l'interface Grafana.

**Requ√™tes types utilis√©es :**

1. **Voir tous les logs de la machine :**
```logql
{job="varlogs"}

```


2. **Filtrer une erreur sp√©cifique (ex: Nginx) :**
```logql
{job="varlogs"} |= "nginx"

```


3. **Retrouver un √©v√©nement marqu√© manuellement :**
```logql
{job="varlogs"} |= "PRESENTATION"

```



### 12.4. Sc√©nario de Validation (Corr√©lation)

Pour prouver le fonctionnement de la cha√Æne compl√®te, nous simulons un √©v√©nement tra√ßable.

1. **G√©n√©ration d'un log manuel sur SRV-APPS :**
```bash
logger "DEBUT_DEMO_LOKI_GRAFANA"

```


2. **V√©rification dans Grafana :**
* Aller dans l'onglet **Explore**.
* S'assurer que la source est **Loki**.
* Lancer la requ√™te `{job="varlogs"}`.
* Le message appara√Æt avec le timestamp pr√©cis, confirmant l'ingestion en temps r√©el.



## üß™ 13. Tableau de Validation Final (Architecture Supervis√©e)

| Service | Machine | Acc√®s / Test | R√©sultat Attendu |
| :--- | :--- | :--- | :--- |
| **Prometheus Targets** | SRV-APPS | `http://192.168.142.11:9090/targets` | 2 Nodes en √©tat **UP** |
| **Grafana Dashboard** | SRV-APPS | `http://192.168.142.11:3000` | Graphiques temps r√©el |
| **Logs (Loki)** | SRV-APPS | Grafana Explore (`{job="varlogs"}`) | Streaming des logs en direct |
| **Alerte CPU** | SRV-APPS | Interface Prometheus (`/alerts`) | √âtat **FIRING** si CPU > 80% |
| **Alerte Disque** | SRV-APPS | Interface Prometheus (`/alerts`) | √âtat **FIRING** si Disque > 95% |
| **Sonde DNS** | SRV-DNS | `systemctl status node_exporter` | √âtat **active (running)** |
| **Persistance** | SRV-APPS | `podman volume inspect grafana_data` | Donn√©es conserv√©es apr√®s reboot |